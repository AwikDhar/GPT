{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07372602-119b-4fc8-8c4e-c8aec2cbc5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import string\n",
    "import random\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop\n",
    "from timeit import timeit\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facc8762-acf5-4332-8247-547aff31cd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"rahular/simple-wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af3f39e-2911-400f-a42f-43dae5792c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 769764\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cdfaa3-0d2f-4c2f-91ed-e4d90158802e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "# wiki['train'] = wiki['train'][:100000]\n",
    "train_size = int(0.9*len(wiki['train']))\n",
    "\n",
    "def wiki_filter(row):\n",
    "    return len(row['text'])>500\n",
    "\n",
    "train = wiki['train'].select(range(train_size)).filter(wiki_filter)\n",
    "test = wiki['train'].select(range(train_size, len(wiki['train']))).filter(wiki_filter)\n",
    "\n",
    "wiki = DatasetDict({\n",
    "    'train' : train,\n",
    "    'test' : test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005cd2f3-80d7-4b53-8753-3d68190445cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 49998\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abdbc78-9bda-4ea7-867b-e998ff851640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(printable chars):\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "stoi = {c:i for i,c in enumerate(sorted(list(string.printable)))}\n",
    "default_int = stoi[' ']\n",
    "print('Vocab(printable chars):\\n',''.join(sorted(stoi.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bdbaf7f-39f9-4f16-a1f0-0a73854d7072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b1771-e7f8-4c3a-a3ee-3d4956702070",
   "metadata": {},
   "source": [
    "### \"Tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5529e14-c249-4503-a92d-fccd3243dadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "            \n",
    "        self.chars = sorted(list(string.printable))\n",
    "        self.itos = {i:c for i,c in enumerate(sorted(list(self.chars)))}\n",
    "        self.stoi = {c:i for i,c in enumerate(sorted(list(self.chars)))}\n",
    "        self.default_int = self.stoi[' '] # space as the default replacement of the unknown char\n",
    "        \n",
    "    def tokenize(self, text: str):\n",
    "        return [self.stoi.get(char, self.default_int) for char in fix_text(text)]\n",
    "    \n",
    "    def decode(self, tokens: List[int]):\n",
    "        return ''.join([self.itos[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed4a4da-7848-40f5-b504-f6cfdf4d70e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d867949b-afb4-4ccb-9f76-564aea293a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 74, 81, 81, 84]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('hello')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "031a1776-a56b-48d0-b9d3-d2aa66b392a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e49214e4-f681-45f6-84f6-1ef0736a185c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir Michael Terence Wogan (; 3 August 1938 – 31 January 2016), better known as Terry Wogan, was a veteran Irish-British radio and television broadcaster, who has worked for the British Broadcasting Corporation in the United Kingdom for most of his career. Before he retired from the weekday breakfast programme \"Wake Up to Wogan\" on BBC Radio 2 on 18 December 2009, Wogan had a regular eight million listeners, making him the most listened to radio broadcaster of any European nation. He began his career at Raidió Teilifís Éireann where he presented shows such as \"Jackpot\" in the 1960s.']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = train.map(tokenizer.tokenize)\n",
    "\n",
    "train.shuffle().select(range(1))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883dfb85-d98c-4e6f-aa92-00d847934f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, context_length, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    data = data.shuffle().select(range(batch_size))\n",
    "    min_data_size = min(len(item['text']) for item in data)\n",
    "    min_data_size = min(min_data_size, context_length)\n",
    "    block_size = random.randint(int(min_data_size*0.5), int(min_data_size*0.8))\n",
    "\n",
    "    data = [tokenizer.tokenize(item['text']) for item in data]\n",
    "\n",
    "    rand_starts = torch.randint(min_data_size-block_size, (batch_size,))\n",
    "    \n",
    "    x, y = torch.empty((0,block_size), dtype=torch.int), torch.empty((0,block_size), dtype=torch.int)\n",
    "    for start, text in zip(rand_starts, data):\n",
    "        # print(torch.tensor(text).unsqueeze(0))\n",
    "        # break\n",
    "        try:\n",
    "            x = torch.cat((x, torch.tensor(text[start:start+block_size]).unsqueeze(0) ), dim = 0)\n",
    "            y = torch.cat((y, torch.tensor(text[start+1:start+block_size+1]).unsqueeze(0) ), dim = 0)    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch creation : {e}\")\n",
    "    \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "278e07c6-04d4-42ee-9790-50549615c825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 455]), torch.Size([2, 455]))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batches(train,2, 1000)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa14819f-ba35-448a-a266-1c71616ecbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768\n",
    "num_heads = 12\n",
    "max_iters = 60000\n",
    "eval_interval = 300\n",
    "eval_iters = 50\n",
    "lr = 0.3e-4\n",
    "dropout = 0.1\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_blocks = 10\n",
    "\n",
    "batch_size = 8\n",
    "context_length = 800\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9502a-3cc5-4200-a59f-25d73746ebab",
   "metadata": {},
   "source": [
    "### Multihead Latent Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6884ce-40a0-4f17-b7ff-129a008d67bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_proj = nn.Linear(embedding_dim, embedding_dim//2)\n",
    "        self.qkv_proj = nn.Linear(embedding_dim//2, embedding_dim*3)\n",
    "        self.o_proj = nn.Linear(head_size*num_heads, embedding_dim)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)).to(device))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        latent = self.latent_proj(x)\n",
    "        qkv = self.qkv_proj(latent)\n",
    "        queries, keys, values = qkv.split(C, dim=-1)\n",
    "        \n",
    "        queries = queries.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        keys = keys.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        values = values.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        \n",
    "        wei = queries@keys.transpose(-2,-1)/(queries.shape[-1]**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, -torch.inf)\n",
    "        weights = F.softmax(wei, dim = -1)\n",
    "        \n",
    "        out = weights@values\n",
    "        # print(self.tril[:T,:T].shape, out.shape)\n",
    "        out = out.transpose(1,2).reshape(B, T, C)\n",
    "        out = self.o_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d141ceb-0db2-4a90-9d5b-5953a80125c3",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13eeb46d-eec6-4edf-ab94-272ab50d507d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim*3)\n",
    "        self.o_proj = nn.Linear(head_size*num_heads, embedding_dim)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)).to(device))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv_proj(x)\n",
    "        queries, keys, values = qkv.split(C, dim=-1)\n",
    "        \n",
    "        queries = queries.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        keys = keys.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        values = values.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        \n",
    "        if kv_cache is not None and not self.training:\n",
    "            past_keys, past_values = kv_cache\n",
    "            keys = torch.cat((past_keys, keys), dim = 2)\n",
    "            values = torch.cat((past_values, values), dim = 2)\n",
    "        \n",
    "        wei = queries@keys.transpose(-2,-1)/(queries.shape[-1]**0.5)\n",
    "        if kv_cache is None:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T]==0, -torch.inf)\n",
    "        weights = F.softmax(wei, dim = -1)\n",
    "        \n",
    "        out = weights@values\n",
    "\n",
    "        out = out.transpose(1,2).reshape(B, T, C)\n",
    "        out = self.o_proj(out)\n",
    "        out = self.dropout(out)\n",
    "            \n",
    "        return (out, (keys,values)) if not self.training else (out, None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fdcbe05-1c95-44df-a1dc-e04988b07b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardVanilla(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(embedding_dim, embedding_dim*8//3, bias=False)\n",
    "        self.silu_proj = nn.Linear(embedding_dim, embedding_dim*8//3, bias=False)\n",
    "        self.down_proj = nn.Linear(embedding_dim*8//3, embedding_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.silu(self.silu_proj(x))*self.up_proj(x)\n",
    "        out = self.down_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f027a6ea-04a0-47a1-8b42-5eab0a833996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = embedding_dim*8//3\n",
    "        self.up_proj = nn.Linear(embedding_dim, self.hidden_dim*2, bias=False)\n",
    "        self.down_proj = nn.Linear(self.hidden_dim, embedding_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.up_proj(x).split(self.hidden_dim, dim = -1) \n",
    "        out = F.silu(x1)*x2\n",
    "        out = self.down_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b4b59979-82fc-4585-95a6-86664c174608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla FFN time:  0.0109s. Parallelized FFN time:  0.0050s. Seed-up :  53.95%\n"
     ]
    }
   ],
   "source": [
    "ed = 4096\n",
    "dev = 'cuda'\n",
    "# x = torch.randn((1,200,ed)).to(dev)\n",
    "\n",
    "# ff_vanilla = FeedForwardVanilla(ed).to(dev)\n",
    "# ff = FeedForward(ed).to(dev)\n",
    "\n",
    "def run_ffn_benchmark(num_trials = 1000):\n",
    "    t1s = 0\n",
    "    t2s = 0\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        t2 = timeit(lambda : ff(x), number = 1)\n",
    "        t1 = timeit(lambda : ff_vanilla(x), number = 1)\n",
    "        \n",
    "        t1s += t1\n",
    "        t2s += t2\n",
    "    \n",
    "    return t1s/num_trials, t2s/num_trials\n",
    "\n",
    "t1, t2 = run_ffn_benchmark()\n",
    "\n",
    "print(f\"Vanilla FFN time: {t1 : .4f}s. Parallelized FFN time: {t2 : .4f}s. Seed-up : {100*(t1-t2)/t1 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376b661d-efa6-490b-bd72-f6ba6a0416d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicTanh(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-4, init_alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.alpha = nn.Parameter(torch.ones(1)*init_alpha)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.alpha*x)\n",
    "        out = self.gamma*out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a792bc8c-db24-4aa8-a954-bfc7c4d9e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = CausalMultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward_net = FeedForward(embedding_dim)\n",
    "        self.dynamic_tanh1 = DynamicTanh(embedding_dim)\n",
    "        self.dynamic_tanh2 = DynamicTanh(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        out, kv_cache = self.multi_head_attention(self.dynamic_tanh1(x), kv_cache)\n",
    "        out = x + out\n",
    "        out = out + self.feed_forward_net(self.dynamic_tanh2(out))\n",
    "        \n",
    "        return out, kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b27d7cf-6239-4118-a3cc-e1b0c7af600a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 64, num_heads: int = 8, num_blocks = 8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dynamic_tanh = DynamicTanh(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, tokens, targets=None, kv_cache = None):\n",
    "\n",
    "        if kv_cache is None:\n",
    "            kv_cache = [None] * len(self.blocks)\n",
    "            out = self.pos_embedding(torch.arange(tokens.shape[-1], device=device)) + self.embedding(tokens)\n",
    "        else:\n",
    "            # trim the kv_cache to keep the context valid\n",
    "            T_past = kv_cache[0][0].shape[2]\n",
    "            if T_past >= context_length:\n",
    "                trim = lambda past_kv : (past_kv[0][:, :, -(context_length-1):, :], past_kv[1][:, :, -(context_length-1):, :])  \n",
    "                kv_cache = [trim(kv_cache[i]) for i in range(len(self.blocks))] \n",
    "            \n",
    "            tokens = tokens[:, [-1]]\n",
    "            out = self.pos_embedding(torch.arange(T_past, T_past+1, device=device)) + self.embedding(tokens)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, updated_block_cache = block(out, kv_cache[i])\n",
    "            kv_cache[i] = updated_block_cache\n",
    "            \n",
    "        out = self.dynamic_tanh(out)\n",
    "        \n",
    "        # If no targets, it is inference and we only care about the last token\n",
    "        if targets is None:    \n",
    "            out = out[:, [-1], :]\n",
    "            \n",
    "        logits = self.lm_head(out)\n",
    "                                 \n",
    "        if targets is None:\n",
    "            return logits, kv_cache\n",
    "            \n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, tokens, temperature = 1, top_k = None, max_new_tokens=100, use_cache = True):\n",
    "        assert temperature>0, \"temperature needs to be positive for comprehensible generations\"\n",
    "        if top_k is not None:\n",
    "            assert top_k>0 and isinstance(top_k, int), \"Non-positive or non-int top_k doesn't make sense!\"\n",
    "        \n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens): \n",
    "            context = tokens[:,-context_length:]\n",
    "            \n",
    "            if use_cache is False:\n",
    "                logits, _ = self(context, None)\n",
    "            else:\n",
    "                logits, kv_cache = self(context, None, kv_cache)\n",
    "            \n",
    "            logits = logits[:,-1,:]/temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                logits = self._get_topk_logits(logits, top_k)\n",
    "                \n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def _get_topk_logits(self, logits, k):\n",
    "        v, _ = torch.topk(logits, k, dim=-1)\n",
    "        min_values = v[:, -1].unsqueeze(-1).expand_as(logits)\n",
    "        \n",
    "        return torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "#     def _get_topk_logits(self, logits, k: int):\n",
    "#         heap = []\n",
    "        \n",
    "#         for logit in logits:\n",
    "#             heappush(heap, logit)\n",
    "#             if len(heap)>k:\n",
    "#                 heappop(heap)\n",
    "           \n",
    "#         logits[logits<heap[0]] = -torch.inf\n",
    "        \n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44802b83-d127-4bd5-8ed3-1a306b4c6f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT(embedding_dim=embedding_dim, num_heads=num_heads, num_blocks=num_blocks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48ed285-a257-4587-b662-602c846548c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(100, 768)\n",
       "  (pos_embedding): Embedding(800, 768)\n",
       "  (blocks): ModuleList(\n",
       "    (0-9): 10 x DecoderBlock(\n",
       "      (multi_head_attention): CausalMultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_net): FeedForward(\n",
       "        (up_proj): Linear(in_features=768, out_features=4096, bias=False)\n",
       "        (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dynamic_tanh1): DynamicTanh()\n",
       "      (dynamic_tanh2): DynamicTanh()\n",
       "    )\n",
       "  )\n",
       "  (dynamic_tanh): DynamicTanh()\n",
       "  (lm_head): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075f1fc4-abc3-4ce9-b4c0-78b4b7fd0885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 71.61M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e6 :.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4088650d-7c75-42e9-8a18-080cc160f55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df821cb-9e95-4945-901f-750e005ab08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_loss = torch.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14736338-eab4-482b-bae1-52ea52958d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"C:/Users/dhars/Documents/Sagemaker notebooks/GPT1-71M-wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b887c431-9380-4eca-93df-2726f88e4606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ecea0d5-64f8-4468-9b92-40c3d7326c9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407f95dd1fa749f2bdf1ebce811c327d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/60000]: Train loss:  1.19\n",
      "[200/60000]: Train loss:  1.28\n",
      "[300/60000]: Train loss:  1.13\n",
      "Eval loss improved:  1.28, saving checkpoint\n",
      "[400/60000]: Train loss:  1.29\n",
      "[500/60000]: Train loss:  1.25\n",
      "[600/60000]: Train loss:  1.32\n",
      "[700/60000]: Train loss:  1.22\n",
      "[800/60000]: Train loss:  1.27\n",
      "[900/60000]: Train loss:  1.31\n",
      "[1000/60000]: Train loss:  1.28\n",
      "[1100/60000]: Train loss:  1.16\n",
      "[1200/60000]: Train loss:  1.21\n",
      "[1300/60000]: Train loss:  1.14\n",
      "[1400/60000]: Train loss:  1.34\n",
      "[1500/60000]: Train loss:  1.26\n",
      "[1600/60000]: Train loss:  1.21\n",
      "[1700/60000]: Train loss:  1.24\n",
      "[1800/60000]: Train loss:  1.21\n",
      "Eval loss improved:  1.26, saving checkpoint\n",
      "[1900/60000]: Train loss:  1.30\n",
      "[2000/60000]: Train loss:  1.23\n",
      "[2100/60000]: Train loss:  1.19\n",
      "Eval loss improved:  1.26, saving checkpoint\n",
      "[2200/60000]: Train loss:  1.33\n",
      "[2300/60000]: Train loss:  1.19\n",
      "[2400/60000]: Train loss:  1.13\n",
      "[2500/60000]: Train loss:  1.24\n",
      "[2600/60000]: Train loss:  1.24\n",
      "[2700/60000]: Train loss:  1.16\n",
      "[2800/60000]: Train loss:  1.34\n",
      "[2900/60000]: Train loss:  1.21\n",
      "[3000/60000]: Train loss:  1.27\n",
      "[3100/60000]: Train loss:  1.19\n",
      "[3200/60000]: Train loss:  1.26\n",
      "[3300/60000]: Train loss:  1.18\n",
      "[3400/60000]: Train loss:  1.30\n",
      "[3500/60000]: Train loss:  1.27\n",
      "[3600/60000]: Train loss:  1.22\n",
      "[3700/60000]: Train loss:  1.22\n",
      "[3800/60000]: Train loss:  1.18\n",
      "[3900/60000]: Train loss:  1.24\n",
      "[4000/60000]: Train loss:  1.30\n",
      "[4100/60000]: Train loss:  1.11\n",
      "[4200/60000]: Train loss:  1.21\n",
      "Eval loss improved:  1.26, saving checkpoint\n",
      "[4300/60000]: Train loss:  1.22\n",
      "[4400/60000]: Train loss:  1.22\n",
      "[4500/60000]: Train loss:  1.23\n",
      "[4600/60000]: Train loss:  1.24\n",
      "[4700/60000]: Train loss:  1.18\n",
      "[4800/60000]: Train loss:  1.28\n",
      "[4900/60000]: Train loss:  1.30\n",
      "[5000/60000]: Train loss:  1.21\n",
      "[5100/60000]: Train loss:  1.13\n",
      "[5200/60000]: Train loss:  1.27\n",
      "[5300/60000]: Train loss:  1.27\n",
      "[5400/60000]: Train loss:  1.22\n",
      "Eval loss improved:  1.25, saving checkpoint\n",
      "[5500/60000]: Train loss:  1.22\n",
      "[5600/60000]: Train loss:  1.20\n",
      "[5700/60000]: Train loss:  1.25\n",
      "[5800/60000]: Train loss:  1.24\n",
      "[5900/60000]: Train loss:  1.18\n",
      "[6000/60000]: Train loss:  1.23\n",
      "[6100/60000]: Train loss:  1.27\n",
      "[6200/60000]: Train loss:  1.24\n",
      "[6300/60000]: Train loss:  1.10\n",
      "Eval loss improved:  1.24, saving checkpoint\n",
      "[6400/60000]: Train loss:  1.15\n",
      "[6500/60000]: Train loss:  1.24\n",
      "[6600/60000]: Train loss:  1.25\n",
      "[6700/60000]: Train loss:  1.27\n",
      "[6800/60000]: Train loss:  1.24\n",
      "[6900/60000]: Train loss:  1.23\n",
      "[7000/60000]: Train loss:  1.15\n",
      "[7100/60000]: Train loss:  1.25\n",
      "[7200/60000]: Train loss:  1.28\n",
      "[7300/60000]: Train loss:  1.21\n",
      "[7400/60000]: Train loss:  1.28\n",
      "[7500/60000]: Train loss:  1.28\n",
      "[7600/60000]: Train loss:  1.14\n",
      "[7700/60000]: Train loss:  1.14\n",
      "[7800/60000]: Train loss:  1.18\n",
      "Eval loss improved:  1.22, saving checkpoint\n",
      "[7900/60000]: Train loss:  1.17\n",
      "[8000/60000]: Train loss:  1.26\n",
      "[8100/60000]: Train loss:  1.21\n",
      "[8200/60000]: Train loss:  1.18\n",
      "[8300/60000]: Train loss:  1.21\n",
      "[8400/60000]: Train loss:  1.30\n",
      "[8500/60000]: Train loss:  1.29\n",
      "[8600/60000]: Train loss:  1.13\n",
      "[8700/60000]: Train loss:  1.24\n",
      "[8800/60000]: Train loss:  1.36\n",
      "[8900/60000]: Train loss:  1.15\n",
      "[9000/60000]: Train loss:  1.20\n",
      "[9100/60000]: Train loss:  1.18\n",
      "[9200/60000]: Train loss:  1.22\n",
      "[9300/60000]: Train loss:  1.21\n",
      "[9400/60000]: Train loss:  1.19\n",
      "[9500/60000]: Train loss:  1.36\n",
      "[9600/60000]: Train loss:  1.11\n",
      "[9700/60000]: Train loss:  1.22\n",
      "[9800/60000]: Train loss:  1.38\n",
      "[9900/60000]: Train loss:  1.30\n",
      "[10000/60000]: Train loss:  1.25\n",
      "[10100/60000]: Train loss:  1.24\n",
      "[10200/60000]: Train loss:  1.21\n",
      "[10300/60000]: Train loss:  1.25\n",
      "[10400/60000]: Train loss:  1.16\n",
      "[10500/60000]: Train loss:  1.20\n",
      "[10600/60000]: Train loss:  1.16\n",
      "[10700/60000]: Train loss:  1.22\n",
      "[10800/60000]: Train loss:  1.23\n",
      "[10900/60000]: Train loss:  1.27\n",
      "[11000/60000]: Train loss:  1.21\n",
      "[11100/60000]: Train loss:  1.22\n",
      "[11200/60000]: Train loss:  1.22\n",
      "[11300/60000]: Train loss:  1.23\n",
      "[11400/60000]: Train loss:  1.18\n",
      "[11500/60000]: Train loss:  1.33\n",
      "[11600/60000]: Train loss:  1.18\n",
      "[11700/60000]: Train loss:  1.18\n",
      "[11800/60000]: Train loss:  1.19\n",
      "[11900/60000]: Train loss:  1.35\n",
      "[12000/60000]: Train loss:  1.23\n",
      "[12100/60000]: Train loss:  1.20\n",
      "[12200/60000]: Train loss:  1.17\n",
      "[12300/60000]: Train loss:  1.15\n",
      "[12400/60000]: Train loss:  1.20\n",
      "[12500/60000]: Train loss:  1.13\n",
      "[12600/60000]: Train loss:  1.23\n",
      "[12700/60000]: Train loss:  1.26\n",
      "[12800/60000]: Train loss:  1.20\n",
      "[12900/60000]: Train loss:  1.17\n",
      "Eval loss improved:  1.22, saving checkpoint\n",
      "[13000/60000]: Train loss:  1.18\n",
      "[13100/60000]: Train loss:  1.26\n",
      "[13200/60000]: Train loss:  1.15\n",
      "[13300/60000]: Train loss:  1.30\n",
      "[13400/60000]: Train loss:  1.19\n",
      "[13500/60000]: Train loss:  1.12\n",
      "[13600/60000]: Train loss:  1.10\n",
      "[13700/60000]: Train loss:  1.19\n",
      "[13800/60000]: Train loss:  1.21\n",
      "[13900/60000]: Train loss:  1.19\n",
      "[14000/60000]: Train loss:  1.19\n",
      "[14100/60000]: Train loss:  1.14\n",
      "Eval loss improved:  1.22, saving checkpoint\n",
      "[14200/60000]: Train loss:  1.17\n",
      "[14300/60000]: Train loss:  1.21\n",
      "[14400/60000]: Train loss:  1.19\n",
      "[14500/60000]: Train loss:  1.14\n",
      "[14600/60000]: Train loss:  1.16\n",
      "[14700/60000]: Train loss:  1.32\n",
      "[14800/60000]: Train loss:  1.21\n",
      "[14900/60000]: Train loss:  1.19\n",
      "[15000/60000]: Train loss:  1.31\n",
      "[15100/60000]: Train loss:  1.27\n",
      "[15200/60000]: Train loss:  1.52\n",
      "[15300/60000]: Train loss:  1.26\n",
      "[15400/60000]: Train loss:  1.18\n",
      "[15500/60000]: Train loss:  1.06\n",
      "[15600/60000]: Train loss:  1.21\n",
      "[15700/60000]: Train loss:  1.19\n",
      "[15800/60000]: Train loss:  1.09\n",
      "[15900/60000]: Train loss:  1.10\n",
      "[16000/60000]: Train loss:  1.15\n",
      "[16100/60000]: Train loss:  1.24\n",
      "[16200/60000]: Train loss:  1.25\n",
      "[16300/60000]: Train loss:  1.24\n",
      "[16400/60000]: Train loss:  1.11\n",
      "[16500/60000]: Train loss:  1.18\n",
      "[16600/60000]: Train loss:  1.20\n",
      "[16700/60000]: Train loss:  1.16\n",
      "[16800/60000]: Train loss:  1.14\n",
      "[16900/60000]: Train loss:  1.23\n",
      "[17000/60000]: Train loss:  1.15\n",
      "[17100/60000]: Train loss:  1.18\n",
      "[17200/60000]: Train loss:  1.18\n",
      "[17300/60000]: Train loss:  1.24\n",
      "[17400/60000]: Train loss:  1.18\n",
      "[17500/60000]: Train loss:  1.05\n",
      "[17600/60000]: Train loss:  1.14\n",
      "[17700/60000]: Train loss:  1.18\n",
      "[17800/60000]: Train loss:  1.21\n",
      "[17900/60000]: Train loss:  1.16\n",
      "[18000/60000]: Train loss:  1.19\n",
      "[18100/60000]: Train loss:  1.19\n",
      "[18200/60000]: Train loss:  1.26\n",
      "[18300/60000]: Train loss:  1.18\n",
      "[18400/60000]: Train loss:  1.13\n",
      "[18500/60000]: Train loss:  1.23\n",
      "[18600/60000]: Train loss:  1.16\n",
      "[18700/60000]: Train loss:  1.19\n",
      "[18800/60000]: Train loss:  1.09\n",
      "[18900/60000]: Train loss:  1.21\n",
      "[19000/60000]: Train loss:  1.12\n",
      "[19100/60000]: Train loss:  1.12\n",
      "[19200/60000]: Train loss:  1.24\n",
      "[19300/60000]: Train loss:  1.23\n",
      "[19400/60000]: Train loss:  1.18\n",
      "[19500/60000]: Train loss:  1.18\n",
      "[19600/60000]: Train loss:  1.21\n",
      "[19700/60000]: Train loss:  1.14\n",
      "[19800/60000]: Train loss:  1.20\n",
      "[19900/60000]: Train loss:  1.25\n",
      "[20000/60000]: Train loss:  1.27\n",
      "[20100/60000]: Train loss:  1.15\n",
      "[20200/60000]: Train loss:  1.11\n",
      "[20300/60000]: Train loss:  1.15\n",
      "[20400/60000]: Train loss:  1.12\n",
      "[20500/60000]: Train loss:  1.19\n",
      "[20600/60000]: Train loss:  1.16\n",
      "[20700/60000]: Train loss:  1.24\n",
      "Eval loss improved:  1.22, saving checkpoint\n",
      "[20800/60000]: Train loss:  1.20\n",
      "[20900/60000]: Train loss:  1.12\n",
      "[21000/60000]: Train loss:  1.20\n",
      "[21100/60000]: Train loss:  1.19\n",
      "[21200/60000]: Train loss:  1.15\n",
      "[21300/60000]: Train loss:  1.19\n",
      "[21400/60000]: Train loss:  1.26\n",
      "[21500/60000]: Train loss:  1.21\n",
      "[21600/60000]: Train loss:  1.23\n",
      "Eval loss improved:  1.21, saving checkpoint\n",
      "[21700/60000]: Train loss:  1.19\n",
      "[21800/60000]: Train loss:  1.14\n",
      "[21900/60000]: Train loss:  1.25\n",
      "[22000/60000]: Train loss:  1.06\n",
      "[22100/60000]: Train loss:  1.16\n",
      "[22200/60000]: Train loss:  1.19\n",
      "[22300/60000]: Train loss:  1.14\n",
      "[22400/60000]: Train loss:  1.25\n",
      "[22500/60000]: Train loss:  1.19\n",
      "[22600/60000]: Train loss:  1.22\n",
      "[22700/60000]: Train loss:  1.18\n",
      "[22800/60000]: Train loss:  1.17\n",
      "Eval loss improved:  1.21, saving checkpoint\n",
      "[22900/60000]: Train loss:  1.15\n",
      "[23000/60000]: Train loss:  1.22\n",
      "[23100/60000]: Train loss:  1.11\n",
      "[23200/60000]: Train loss:  1.11\n",
      "[23300/60000]: Train loss:  1.18\n",
      "[23400/60000]: Train loss:  1.31\n",
      "[23500/60000]: Train loss:  1.21\n",
      "[23600/60000]: Train loss:  1.13\n",
      "[23700/60000]: Train loss:  1.13\n",
      "[23800/60000]: Train loss:  1.06\n",
      "[23900/60000]: Train loss:  1.27\n",
      "[24000/60000]: Train loss:  1.12\n",
      "[24100/60000]: Train loss:  1.05\n",
      "[24200/60000]: Train loss:  1.17\n",
      "[24300/60000]: Train loss:  1.17\n",
      "[24400/60000]: Train loss:  1.21\n",
      "[24500/60000]: Train loss:  1.15\n",
      "[24600/60000]: Train loss:  1.11\n",
      "[24700/60000]: Train loss:  1.16\n",
      "[24800/60000]: Train loss:  1.13\n",
      "[24900/60000]: Train loss:  1.18\n",
      "[25000/60000]: Train loss:  1.09\n",
      "[25100/60000]: Train loss:  1.14\n",
      "[25200/60000]: Train loss:  1.12\n",
      "[25300/60000]: Train loss:  1.21\n",
      "[25400/60000]: Train loss:  1.13\n",
      "[25500/60000]: Train loss:  1.18\n",
      "[25600/60000]: Train loss:  1.15\n",
      "[25700/60000]: Train loss:  1.19\n",
      "[25800/60000]: Train loss:  1.07\n",
      "[25900/60000]: Train loss:  1.22\n",
      "[26000/60000]: Train loss:  1.19\n",
      "[26100/60000]: Train loss:  1.12\n",
      "[26200/60000]: Train loss:  1.25\n",
      "[26300/60000]: Train loss:  1.13\n",
      "[26400/60000]: Train loss:  1.13\n",
      "[26500/60000]: Train loss:  1.13\n",
      "[26600/60000]: Train loss:  1.35\n",
      "[26700/60000]: Train loss:  1.22\n",
      "[26800/60000]: Train loss:  1.06\n",
      "[26900/60000]: Train loss:  1.17\n",
      "[27000/60000]: Train loss:  1.11\n",
      "[27100/60000]: Train loss:  1.09\n",
      "[27200/60000]: Train loss:  1.22\n",
      "[27300/60000]: Train loss:  1.07\n",
      "Eval loss improved:  1.20, saving checkpoint\n",
      "[27400/60000]: Train loss:  1.18\n",
      "[27500/60000]: Train loss:  1.11\n",
      "[27600/60000]: Train loss:  1.16\n",
      "[27700/60000]: Train loss:  1.36\n",
      "[27800/60000]: Train loss:  1.07\n",
      "[27900/60000]: Train loss:  1.12\n",
      "Eval loss improved:  1.19, saving checkpoint\n",
      "[28000/60000]: Train loss:  1.12\n",
      "[28100/60000]: Train loss:  1.20\n",
      "[28200/60000]: Train loss:  1.21\n",
      "[28300/60000]: Train loss:  1.18\n",
      "[28400/60000]: Train loss:  1.12\n",
      "[28500/60000]: Train loss:  1.15\n",
      "Eval loss improved:  1.19, saving checkpoint\n",
      "[28600/60000]: Train loss:  1.17\n",
      "[28700/60000]: Train loss:  1.19\n",
      "[28800/60000]: Train loss:  1.18\n",
      "[28900/60000]: Train loss:  1.16\n",
      "[29000/60000]: Train loss:  1.10\n",
      "[29100/60000]: Train loss:  1.23\n",
      "[29200/60000]: Train loss:  1.15\n",
      "[29300/60000]: Train loss:  1.14\n",
      "[29400/60000]: Train loss:  1.13\n",
      "[29500/60000]: Train loss:  1.22\n",
      "[29600/60000]: Train loss:  1.15\n",
      "[29700/60000]: Train loss:  1.29\n",
      "[29800/60000]: Train loss:  1.15\n",
      "[29900/60000]: Train loss:  1.18\n",
      "[30000/60000]: Train loss:  1.25\n",
      "[30100/60000]: Train loss:  1.21\n",
      "[30200/60000]: Train loss:  1.12\n",
      "[30300/60000]: Train loss:  1.20\n",
      "[30400/60000]: Train loss:  1.12\n",
      "[30500/60000]: Train loss:  1.20\n",
      "[30600/60000]: Train loss:  1.18\n",
      "[30700/60000]: Train loss:  1.19\n",
      "[30800/60000]: Train loss:  1.18\n",
      "[30900/60000]: Train loss:  1.12\n",
      "[31000/60000]: Train loss:  1.23\n",
      "[31100/60000]: Train loss:  1.09\n",
      "[31200/60000]: Train loss:  1.25\n",
      "[31300/60000]: Train loss:  1.30\n",
      "[31400/60000]: Train loss:  1.16\n",
      "[31500/60000]: Train loss:  1.14\n",
      "[31600/60000]: Train loss:  1.14\n",
      "[31700/60000]: Train loss:  1.18\n",
      "[31800/60000]: Train loss:  1.12\n",
      "Eval loss improved:  1.19, saving checkpoint\n",
      "[31900/60000]: Train loss:  1.15\n",
      "[32000/60000]: Train loss:  1.14\n",
      "[32100/60000]: Train loss:  1.10\n",
      "Eval loss improved:  1.18, saving checkpoint\n",
      "[32200/60000]: Train loss:  1.10\n",
      "[32300/60000]: Train loss:  1.09\n",
      "[32400/60000]: Train loss:  1.21\n",
      "[32500/60000]: Train loss:  1.27\n",
      "[32600/60000]: Train loss:  1.14\n",
      "[32700/60000]: Train loss:  1.19\n",
      "[32800/60000]: Train loss:  1.09\n",
      "[32900/60000]: Train loss:  1.15\n",
      "[33000/60000]: Train loss:  1.17\n",
      "[33100/60000]: Train loss:  1.12\n",
      "[33200/60000]: Train loss:  1.19\n",
      "[33300/60000]: Train loss:  1.24\n",
      "[33400/60000]: Train loss:  1.09\n",
      "[33500/60000]: Train loss:  1.23\n",
      "[33600/60000]: Train loss:  1.20\n",
      "[33700/60000]: Train loss:  1.11\n",
      "[33800/60000]: Train loss:  1.20\n",
      "[33900/60000]: Train loss:  1.15\n",
      "[34000/60000]: Train loss:  1.15\n",
      "[34100/60000]: Train loss:  1.15\n",
      "[34200/60000]: Train loss:  1.25\n",
      "[34300/60000]: Train loss:  1.19\n",
      "[34400/60000]: Train loss:  1.02\n",
      "[34500/60000]: Train loss:  1.10\n",
      "[34600/60000]: Train loss:  1.28\n",
      "[34700/60000]: Train loss:  1.21\n",
      "[34800/60000]: Train loss:  1.14\n",
      "[34900/60000]: Train loss:  1.14\n",
      "[35000/60000]: Train loss:  1.15\n",
      "[35100/60000]: Train loss:  1.22\n",
      "[35200/60000]: Train loss:  1.19\n",
      "[35300/60000]: Train loss:  1.13\n",
      "[35400/60000]: Train loss:  1.11\n",
      "[35500/60000]: Train loss:  1.16\n",
      "[35600/60000]: Train loss:  1.12\n",
      "[35700/60000]: Train loss:  1.16\n",
      "[35800/60000]: Train loss:  1.20\n",
      "[35900/60000]: Train loss:  1.06\n",
      "[36000/60000]: Train loss:  1.06\n",
      "[36100/60000]: Train loss:  1.16\n",
      "[36200/60000]: Train loss:  1.14\n",
      "[36300/60000]: Train loss:  1.05\n",
      "[36400/60000]: Train loss:  1.13\n",
      "[36500/60000]: Train loss:  1.17\n",
      "[36600/60000]: Train loss:  1.14\n",
      "[36700/60000]: Train loss:  1.18\n",
      "[36800/60000]: Train loss:  1.12\n",
      "[36900/60000]: Train loss:  1.25\n",
      "Eval loss improved:  1.18, saving checkpoint\n",
      "[37000/60000]: Train loss:  1.14\n",
      "[37100/60000]: Train loss:  1.13\n",
      "[37200/60000]: Train loss:  1.08\n",
      "[37300/60000]: Train loss:  1.12\n",
      "[37400/60000]: Train loss:  1.15\n",
      "[37500/60000]: Train loss:  1.14\n",
      "[37600/60000]: Train loss:  1.17\n",
      "[37700/60000]: Train loss:  1.11\n",
      "[37800/60000]: Train loss:  1.16\n",
      "[37900/60000]: Train loss:  1.11\n",
      "[38000/60000]: Train loss:  1.24\n",
      "[38100/60000]: Train loss:  1.24\n",
      "[38200/60000]: Train loss:  1.24\n",
      "[38300/60000]: Train loss:  1.26\n",
      "[38400/60000]: Train loss:  1.09\n",
      "[38500/60000]: Train loss:  1.29\n",
      "[38600/60000]: Train loss:  1.10\n",
      "[38700/60000]: Train loss:  1.19\n",
      "[38800/60000]: Train loss:  1.08\n",
      "[38900/60000]: Train loss:  1.12\n",
      "[39000/60000]: Train loss:  1.06\n",
      "[39100/60000]: Train loss:  1.14\n",
      "[39200/60000]: Train loss:  1.23\n",
      "[39300/60000]: Train loss:  1.14\n",
      "[39400/60000]: Train loss:  1.08\n",
      "[39500/60000]: Train loss:  1.24\n",
      "[39600/60000]: Train loss:  1.18\n",
      "Eval loss improved:  1.18, saving checkpoint\n",
      "[39700/60000]: Train loss:  1.16\n",
      "[39800/60000]: Train loss:  1.22\n",
      "[39900/60000]: Train loss:  1.09\n",
      "[40000/60000]: Train loss:  1.10\n",
      "[40100/60000]: Train loss:  1.01\n",
      "[40200/60000]: Train loss:  1.07\n",
      "[40300/60000]: Train loss:  1.12\n",
      "[40400/60000]: Train loss:  1.11\n",
      "[40500/60000]: Train loss:  1.14\n",
      "[40600/60000]: Train loss:  1.13\n",
      "[40700/60000]: Train loss:  1.14\n",
      "[40800/60000]: Train loss:  1.18\n",
      "[40900/60000]: Train loss:  1.08\n",
      "[41000/60000]: Train loss:  1.12\n",
      "[41100/60000]: Train loss:  1.13\n",
      "Eval loss improved:  1.18, saving checkpoint\n",
      "[41200/60000]: Train loss:  1.09\n",
      "[41300/60000]: Train loss:  1.03\n",
      "[41400/60000]: Train loss:  1.10\n",
      "[41500/60000]: Train loss:  1.13\n",
      "[41600/60000]: Train loss:  1.11\n",
      "[41700/60000]: Train loss:  1.09\n",
      "[41800/60000]: Train loss:  1.26\n",
      "[41900/60000]: Train loss:  1.02\n",
      "[42000/60000]: Train loss:  1.06\n",
      "[42100/60000]: Train loss:  1.12\n",
      "[42200/60000]: Train loss:  1.14\n",
      "[42300/60000]: Train loss:  1.13\n",
      "[42400/60000]: Train loss:  1.22\n",
      "[42500/60000]: Train loss:  1.23\n",
      "[42600/60000]: Train loss:  1.24\n",
      "[42700/60000]: Train loss:  1.09\n",
      "[42800/60000]: Train loss:  1.14\n",
      "[42900/60000]: Train loss:  1.15\n",
      "[43000/60000]: Train loss:  1.12\n",
      "[43100/60000]: Train loss:  1.05\n",
      "[43200/60000]: Train loss:  1.21\n",
      "[43300/60000]: Train loss:  1.20\n",
      "[43400/60000]: Train loss:  1.12\n",
      "[43500/60000]: Train loss:  1.24\n",
      "[43600/60000]: Train loss:  1.11\n",
      "[43700/60000]: Train loss:  1.16\n",
      "[43800/60000]: Train loss:  1.17\n",
      "[43900/60000]: Train loss:  1.19\n",
      "[44000/60000]: Train loss:  1.22\n",
      "[44100/60000]: Train loss:  1.04\n",
      "[44200/60000]: Train loss:  1.11\n",
      "[44300/60000]: Train loss:  1.14\n",
      "[44400/60000]: Train loss:  1.22\n",
      "[44500/60000]: Train loss:  1.05\n",
      "[44600/60000]: Train loss:  1.12\n",
      "[44700/60000]: Train loss:  1.20\n",
      "[44800/60000]: Train loss:  1.18\n",
      "[44900/60000]: Train loss:  1.02\n",
      "[45000/60000]: Train loss:  1.07\n",
      "[45100/60000]: Train loss:  1.21\n",
      "[45200/60000]: Train loss:  1.17\n",
      "[45300/60000]: Train loss:  1.05\n",
      "[45400/60000]: Train loss:  1.11\n",
      "[45500/60000]: Train loss:  1.16\n",
      "[45600/60000]: Train loss:  1.13\n",
      "[45700/60000]: Train loss:  1.10\n",
      "[45800/60000]: Train loss:  1.13\n",
      "[45900/60000]: Train loss:  1.29\n",
      "[46000/60000]: Train loss:  1.12\n",
      "[46100/60000]: Train loss:  1.05\n",
      "[46200/60000]: Train loss:  1.17\n",
      "[46300/60000]: Train loss:  1.21\n",
      "[46400/60000]: Train loss:  1.12\n",
      "[46500/60000]: Train loss:  1.12\n",
      "Eval loss improved:  1.16, saving checkpoint\n",
      "[46600/60000]: Train loss:  1.28\n",
      "[46700/60000]: Train loss:  1.11\n",
      "[46800/60000]: Train loss:  1.09\n",
      "[46900/60000]: Train loss:  1.11\n",
      "[47000/60000]: Train loss:  1.17\n",
      "[47100/60000]: Train loss:  1.16\n",
      "[47200/60000]: Train loss:  1.17\n",
      "[47300/60000]: Train loss:  1.17\n",
      "[47400/60000]: Train loss:  1.06\n",
      "[47500/60000]: Train loss:  1.05\n",
      "[47600/60000]: Train loss:  1.07\n",
      "[47700/60000]: Train loss:  1.11\n",
      "[47800/60000]: Train loss:  1.13\n",
      "[47900/60000]: Train loss:  1.05\n",
      "[48000/60000]: Train loss:  1.10\n",
      "Eval loss improved:  1.16, saving checkpoint\n",
      "[48100/60000]: Train loss:  1.15\n",
      "[48200/60000]: Train loss:  1.22\n",
      "[48300/60000]: Train loss:  1.12\n",
      "[48400/60000]: Train loss:  1.06\n",
      "[48500/60000]: Train loss:  1.07\n",
      "[48600/60000]: Train loss:  1.18\n",
      "[48700/60000]: Train loss:  1.20\n",
      "[48800/60000]: Train loss:  1.12\n",
      "[48900/60000]: Train loss:  1.15\n",
      "[49000/60000]: Train loss:  1.12\n",
      "[49100/60000]: Train loss:  1.05\n",
      "[49200/60000]: Train loss:  1.08\n",
      "[49300/60000]: Train loss:  1.13\n",
      "[49400/60000]: Train loss:  1.24\n",
      "[49500/60000]: Train loss:  1.06\n",
      "[49600/60000]: Train loss:  1.17\n",
      "[49700/60000]: Train loss:  1.05\n",
      "[49800/60000]: Train loss:  1.12\n",
      "[49900/60000]: Train loss:  1.16\n",
      "[50000/60000]: Train loss:  1.21\n",
      "[50100/60000]: Train loss:  1.16\n",
      "[50200/60000]: Train loss:  1.13\n",
      "[50300/60000]: Train loss:  1.30\n",
      "[50400/60000]: Train loss:  1.03\n",
      "[50500/60000]: Train loss:  1.01\n",
      "[50600/60000]: Train loss:  1.09\n",
      "[50700/60000]: Train loss:  1.16\n",
      "[50800/60000]: Train loss:  1.08\n",
      "[50900/60000]: Train loss:  1.07\n",
      "[51000/60000]: Train loss:  1.10\n",
      "[51100/60000]: Train loss:  1.02\n",
      "[51200/60000]: Train loss:  1.16\n",
      "[51300/60000]: Train loss:  1.09\n",
      "[51400/60000]: Train loss:  1.15\n",
      "[51500/60000]: Train loss:  1.20\n",
      "[51600/60000]: Train loss:  1.18\n",
      "[51700/60000]: Train loss:  1.18\n",
      "[51800/60000]: Train loss:  1.10\n",
      "[51900/60000]: Train loss:  1.16\n",
      "[52000/60000]: Train loss:  1.11\n",
      "[52100/60000]: Train loss:  1.10\n",
      "[52200/60000]: Train loss:  1.05\n",
      "[52300/60000]: Train loss:  1.07\n",
      "[52400/60000]: Train loss:  1.17\n",
      "[52500/60000]: Train loss:  1.20\n",
      "[52600/60000]: Train loss:  1.10\n",
      "[52700/60000]: Train loss:  1.16\n",
      "[52800/60000]: Train loss:  1.16\n",
      "[52900/60000]: Train loss:  1.18\n",
      "[53000/60000]: Train loss:  1.18\n",
      "[53100/60000]: Train loss:  1.09\n",
      "[53200/60000]: Train loss:  1.06\n",
      "[53300/60000]: Train loss:  1.07\n",
      "[53400/60000]: Train loss:  1.11\n",
      "[53500/60000]: Train loss:  1.09\n",
      "[53600/60000]: Train loss:  1.09\n",
      "[53700/60000]: Train loss:  1.11\n",
      "[53800/60000]: Train loss:  1.13\n",
      "[53900/60000]: Train loss:  1.13\n",
      "[54000/60000]: Train loss:  1.12\n",
      "[54100/60000]: Train loss:  1.13\n",
      "[54200/60000]: Train loss:  1.06\n",
      "[54300/60000]: Train loss:  1.03\n",
      "[54400/60000]: Train loss:  1.09\n",
      "[54500/60000]: Train loss:  1.11\n",
      "[54600/60000]: Train loss:  1.12\n",
      "[54700/60000]: Train loss:  1.17\n",
      "[54800/60000]: Train loss:  1.15\n",
      "[54900/60000]: Train loss:  1.16\n",
      "[55000/60000]: Train loss:  1.05\n",
      "[55100/60000]: Train loss:  1.12\n",
      "[55200/60000]: Train loss:  1.12\n",
      "[55300/60000]: Train loss:  1.15\n",
      "[55400/60000]: Train loss:  1.06\n",
      "[55500/60000]: Train loss:  1.09\n",
      "[55600/60000]: Train loss:  1.07\n",
      "[55700/60000]: Train loss:  1.14\n",
      "[55800/60000]: Train loss:  1.11\n",
      "[55900/60000]: Train loss:  1.09\n",
      "[56000/60000]: Train loss:  1.18\n",
      "[56100/60000]: Train loss:  1.14\n",
      "[56200/60000]: Train loss:  1.10\n",
      "[56300/60000]: Train loss:  1.12\n",
      "[56400/60000]: Train loss:  1.21\n",
      "[56500/60000]: Train loss:  1.08\n",
      "[56600/60000]: Train loss:  1.14\n",
      "[56700/60000]: Train loss:  0.98\n",
      "[56800/60000]: Train loss:  1.08\n",
      "[56900/60000]: Train loss:  1.22\n",
      "[57000/60000]: Train loss:  1.10\n",
      "[57100/60000]: Train loss:  1.15\n",
      "[57200/60000]: Train loss:  1.01\n",
      "[57300/60000]: Train loss:  1.14\n",
      "[57400/60000]: Train loss:  1.13\n",
      "[57500/60000]: Train loss:  1.13\n",
      "[57600/60000]: Train loss:  1.21\n",
      "Eval loss improved:  1.16, saving checkpoint\n",
      "[57700/60000]: Train loss:  1.08\n",
      "[57800/60000]: Train loss:  1.06\n",
      "[57900/60000]: Train loss:  1.13\n",
      "[58000/60000]: Train loss:  1.02\n",
      "[58100/60000]: Train loss:  1.04\n",
      "[58200/60000]: Train loss:  1.15\n",
      "[58300/60000]: Train loss:  1.14\n",
      "[58400/60000]: Train loss:  0.98\n",
      "[58500/60000]: Train loss:  1.08\n",
      "[58600/60000]: Train loss:  1.14\n",
      "[58700/60000]: Train loss:  1.21\n",
      "[58800/60000]: Train loss:  1.18\n",
      "[58900/60000]: Train loss:  1.12\n",
      "[59000/60000]: Train loss:  1.08\n",
      "[59100/60000]: Train loss:  1.07\n",
      "Eval loss improved:  1.16, saving checkpoint\n",
      "[59200/60000]: Train loss:  1.20\n",
      "[59300/60000]: Train loss:  1.04\n",
      "[59400/60000]: Train loss:  1.20\n",
      "[59500/60000]: Train loss:  1.10\n",
      "[59600/60000]: Train loss:  1.04\n",
      "[59700/60000]: Train loss:  1.20\n",
      "[59800/60000]: Train loss:  1.15\n",
      "[59900/60000]: Train loss:  1.07\n",
      "[60000/60000]: Train loss:  1.13\n"
     ]
    }
   ],
   "source": [
    "for iter_ in tqdm(range(1, max_iters+1), colour='green'):\n",
    "    model.train()\n",
    "    \n",
    "    x,y = get_batches(data=train, batch_size=batch_size, context_length=context_length, device=device)    \n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(tokens=x,targets=y)\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter_%100==0:\n",
    "        print(f\"[{iter_}/{max_iters}]: Train loss: {loss.mean(): .2f}\")\n",
    "    if iter_%eval_interval==0 or iter_==max_iters:\n",
    "        model.eval()\n",
    "        eval_losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(eval_iters):\n",
    "                x,y = get_batches(test, batch_size, context_length, device)\n",
    "                logits, loss = model(x,y)\n",
    "                eval_losses[i] = loss\n",
    "            eval_loss = eval_losses.mean()\n",
    "            if eval_loss<min_loss:\n",
    "                min_loss = eval_loss\n",
    "                print(f\"Eval loss improved: {eval_loss: .2f}, saving checkpoint\")\n",
    "                torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52535c4f-ed5d-473c-9ca0-3ab0ebd4dce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 279]), torch.Size([8, 279]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_batches(train, batch_size, context_length, device)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3273058b-6211-4392-96c1-ba725c73be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e14a486-4b2e-41fc-8430-0086ec061c2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gpt_response(prompt: str, max_new_tokens: int =400, use_cache = True) -> str:\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = torch.tensor(tokenizer.tokenize(prompt), device=device).unsqueeze(0)\n",
    "    out = tokenizer.decode(model.generate(prompt, max_new_tokens=max_new_tokens, use_cache=use_cache)[0].tolist())\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7911e33-5da8-4069-88b2-d4d775c16c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian subcontinent along the Indianapolis River, which is perfectorial that have brothers above the city. There are three known types of mines in the north part of the individual area, they are said to his collaborator, motion laws about them to function that only circumstances between these collaborators were used to track them. During the 16th century, the Free River river broke up and to be ranked into the colla\n"
     ]
    }
   ],
   "source": [
    "print(get_gpt_response(\"Indian subcontinent\", max_new_tokens=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5420f095-d3ae-4fdf-83f6-54c8d442d1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian subcontinent folk pattern. A recording that affect them was written focusing over commercial that normally surrendered by the European country. Commercial thermometers were also writing commercial or psychology. Under other Union, in the corresponding being harder for prison. The levings were prevailed from the Federal mountains. This victory was loosely given smart by being ambulance at night. The mountain retaining must passed spread in according to the storm of immunic havinghst in crearrdsw alsoom dere \n"
     ]
    }
   ],
   "source": [
    "print(get_gpt_response(\"Indian subcontinent\", max_new_tokens=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba45db3-1e16-4d21-9917-ad30d9813396",
   "metadata": {},
   "source": [
    "### KV cache speed up on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bdae6b79-5e1d-4c74-ae68-cedc7645c247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.247228200081736"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=100), number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fe51c3ed-c724-4aa8-9bb2-4da175b3afbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.07912310003303"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=100, use_cache=False), number=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de4d70-440a-44f8-9d3b-72a2c7d72811",
   "metadata": {},
   "source": [
    "### KV cache speed up on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68fe9b00-9536-4877-96cb-8b659ba71ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.50981409987435"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=750), number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a956c2-69e2-4253-b9e2-d72d187bf689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.67140390002169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=750, use_cache=False), number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3105bf3-2dce-47fa-bdcb-efba6c6f512a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(tokens, max_new_tokens=100, temperature=0.1):\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = tokens[:,-context_length:]\n",
    "        logits = model(context)\n",
    "        logits = logits[:,-1,:]\n",
    "        probabilities = F.softmax(logits/temperature, dim=1)\n",
    "        next_token = torch.multinomial(probabilities, 1)\n",
    "        tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c316231-3e8f-42cf-98b4-03b4478f802f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = generate(prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "77e173b3-4ed1-46e3-98ac-6c07dde1fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUTUS:\n",
      "The stand the father the father of the hands:\n",
      "The shall be the some the some of the the some\n",
      "That \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8210e-b61f-478a-adde-bf49d781a255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
