{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07372602-119b-4fc8-8c4e-c8aec2cbc5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import string\n",
    "import random\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "from heapq import heappush, heappop\n",
    "from timeit import timeit\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facc8762-acf5-4332-8247-547aff31cd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"rahular/simple-wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af3f39e-2911-400f-a42f-43dae5792c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 769764\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cdfaa3-0d2f-4c2f-91ed-e4d90158802e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "train_size = int(0.9*len(wiki['train']))\n",
    "\n",
    "def wiki_filter(row):\n",
    "    return len(row['text'])>500\n",
    "\n",
    "train = wiki['train'].select(range(train_size)).filter(wiki_filter)\n",
    "test = wiki['train'].select(range(train_size, len(wiki['train']))).filter(wiki_filter)\n",
    "\n",
    "wiki = DatasetDict({\n",
    "    'train' : train,\n",
    "    'test' : test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005cd2f3-80d7-4b53-8753-3d68190445cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 49998\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abdbc78-9bda-4ea7-867b-e998ff851640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(printable chars):\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "stoi = {c:i for i,c in enumerate(sorted(list(string.printable)))}\n",
    "default_int = stoi[' ']\n",
    "print('Vocab(printable chars):\\n',''.join(sorted(stoi.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bdbaf7f-39f9-4f16-a1f0-0a73854d7072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b1771-e7f8-4c3a-a3ee-3d4956702070",
   "metadata": {},
   "source": [
    "### \"Tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5529e14-c249-4503-a92d-fccd3243dadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "            \n",
    "        self.chars = sorted(list(string.printable))\n",
    "        self.itos = {i:c for i,c in enumerate(sorted(list(self.chars)))}\n",
    "        self.stoi = {c:i for i,c in enumerate(sorted(list(self.chars)))}\n",
    "        self.default_int = self.stoi[' '] # space as the default replacement of the unknown char\n",
    "        \n",
    "    def tokenize(self, text: str):\n",
    "        return [self.stoi.get(char, self.default_int) for char in fix_text(text)]\n",
    "    \n",
    "    def decode(self, tokens: List[int]):\n",
    "        return ''.join([self.itos[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed4a4da-7848-40f5-b504-f6cfdf4d70e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d867949b-afb4-4ccb-9f76-564aea293a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 74, 81, 81, 84]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('hello')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "031a1776-a56b-48d0-b9d3-d2aa66b392a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e49214e4-f681-45f6-84f6-1ef0736a185c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir Michael Terence Wogan (; 3 August 1938 – 31 January 2016), better known as Terry Wogan, was a veteran Irish-British radio and television broadcaster, who has worked for the British Broadcasting Corporation in the United Kingdom for most of his career. Before he retired from the weekday breakfast programme \"Wake Up to Wogan\" on BBC Radio 2 on 18 December 2009, Wogan had a regular eight million listeners, making him the most listened to radio broadcaster of any European nation. He began his career at Raidió Teilifís Éireann where he presented shows such as \"Jackpot\" in the 1960s.']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = train.map(tokenizer.tokenize)\n",
    "\n",
    "train.shuffle().select(range(1))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883dfb85-d98c-4e6f-aa92-00d847934f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, context_length, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    data = data.shuffle().select(range(batch_size))\n",
    "    min_data_size = min(len(item['text']) for item in data)\n",
    "    min_data_size = min(min_data_size, context_length)\n",
    "    block_size = random.randint(int(min_data_size*0.5), int(min_data_size*0.8))\n",
    "\n",
    "    data = [tokenizer.tokenize(item['text']) for item in data]\n",
    "\n",
    "    rand_starts = torch.randint(min_data_size-block_size, (batch_size,))\n",
    "    \n",
    "    x, y = torch.empty((0,block_size), dtype=torch.int), torch.empty((0,block_size), dtype=torch.int)\n",
    "    for start, text in zip(rand_starts, data):\n",
    "        try:\n",
    "            x = torch.cat((x, torch.tensor(text[start:start+block_size]).unsqueeze(0) ), dim = 0)\n",
    "            y = torch.cat((y, torch.tensor(text[start+1:start+block_size+1]).unsqueeze(0) ), dim = 0)    \n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch creation : {e}\")\n",
    "    \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "278e07c6-04d4-42ee-9790-50549615c825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 455]), torch.Size([2, 455]))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batches(train,2, 1000)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa14819f-ba35-448a-a266-1c71616ecbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768\n",
    "num_heads = 12\n",
    "max_iters = 60000\n",
    "eval_interval = 300\n",
    "eval_iters = 50\n",
    "lr = 0.3e-4\n",
    "dropout = 0.1\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_blocks = 10\n",
    "\n",
    "batch_size = 8\n",
    "context_length = 800\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9502a-3cc5-4200-a59f-25d73746ebab",
   "metadata": {},
   "source": [
    "### Multihead Latent Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6884ce-40a0-4f17-b7ff-129a008d67bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_proj = nn.Linear(embedding_dim, embedding_dim//2)\n",
    "        self.qkv_proj = nn.Linear(embedding_dim//2, embedding_dim*3)\n",
    "        self.o_proj = nn.Linear(head_size*num_heads, embedding_dim)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)).to(device))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        latent = self.latent_proj(x)\n",
    "        qkv = self.qkv_proj(latent)\n",
    "        queries, keys, values = qkv.split(C, dim=-1)\n",
    "        \n",
    "        queries = queries.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        keys = keys.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        values = values.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        \n",
    "        wei = queries@keys.transpose(-2,-1)/(queries.shape[-1]**0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, -torch.inf)\n",
    "        weights = F.softmax(wei, dim = -1)\n",
    "        \n",
    "        out = weights@values\n",
    "\n",
    "        out = out.transpose(1,2).reshape(B, T, C)\n",
    "        out = self.o_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d141ceb-0db2-4a90-9d5b-5953a80125c3",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13eeb46d-eec6-4edf-ab94-272ab50d507d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim*3)\n",
    "        self.o_proj = nn.Linear(head_size*num_heads, embedding_dim)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)).to(device))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv_proj(x)\n",
    "        queries, keys, values = qkv.split(C, dim=-1)\n",
    "        \n",
    "        queries = queries.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        keys = keys.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        values = values.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
    "        \n",
    "        if kv_cache is not None and not self.training:\n",
    "            past_keys, past_values = kv_cache\n",
    "            keys = torch.cat((past_keys, keys), dim = 2)\n",
    "            values = torch.cat((past_values, values), dim = 2)\n",
    "        \n",
    "        wei = queries@keys.transpose(-2,-1)/(queries.shape[-1]**0.5)\n",
    "        # Masking only if kv cache is None, no concept of masking when only new/last token is passed(kv_cache exists)\n",
    "        if kv_cache is None:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T]==0, -torch.inf)\n",
    "        weights = F.softmax(wei, dim = -1)\n",
    "        \n",
    "        out = weights@values\n",
    "\n",
    "        out = out.transpose(1,2).reshape(B, T, C)\n",
    "        out = self.o_proj(out)\n",
    "        out = self.dropout(out)\n",
    "            \n",
    "        return (out, (keys,values)) if not self.training else (out, None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdcbe05-1c95-44df-a1dc-e04988b07b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardVanilla(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(embedding_dim, embedding_dim*8//3, bias=False)\n",
    "        self.silu_proj = nn.Linear(embedding_dim, embedding_dim*8//3, bias=False)\n",
    "        self.down_proj = nn.Linear(embedding_dim*8//3, embedding_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.silu(self.silu_proj(x))*self.up_proj(x)\n",
    "        out = self.down_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f027a6ea-04a0-47a1-8b42-5eab0a833996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = embedding_dim*8//3\n",
    "        self.up_proj = nn.Linear(embedding_dim, self.hidden_dim*2, bias=False)\n",
    "        self.down_proj = nn.Linear(self.hidden_dim, embedding_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.up_proj(x).split(self.hidden_dim, dim = -1) \n",
    "        out = F.silu(x1)*x2\n",
    "        out = self.down_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0647c-cb28-4d61-b34a-418806f61f9b",
   "metadata": {},
   "source": [
    "### Parallelized SwiGLU benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b59979-82fc-4585-95a6-86664c174608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla FFN time:  0.0108s. Parallelized FFN time:  0.0050s. Speed-up :  53.84%\n"
     ]
    }
   ],
   "source": [
    "ed = 4096\n",
    "dev = 'cuda'\n",
    "\n",
    "x = torch.randn((1,200,ed)).to(dev)\n",
    "\n",
    "ff_vanilla = FeedForwardVanilla(ed).to(dev)\n",
    "ff = FeedForward(ed).to(dev)\n",
    "\n",
    "def run_ffn_benchmark(num_trials = 1000):\n",
    "    t1s = 0\n",
    "    t2s = 0\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        t2 = timeit(lambda : ff(x), number = 1)\n",
    "        t1 = timeit(lambda : ff_vanilla(x), number = 1)\n",
    "        \n",
    "        t1s += t1\n",
    "        t2s += t2\n",
    "    \n",
    "    return t1s/num_trials, t2s/num_trials\n",
    "\n",
    "t1, t2 = run_ffn_benchmark()\n",
    "\n",
    "print(f\"Vanilla FFN time: {t1 : .4f}s. Parallelized FFN time: {t2 : .4f}s. Speed-up : {100*(t1-t2)/t1 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376b661d-efa6-490b-bd72-f6ba6a0416d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicTanh(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-4, init_alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.alpha = nn.Parameter(torch.ones(1)*init_alpha)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.alpha*x)\n",
    "        out = self.gamma*out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a792bc8c-db24-4aa8-a954-bfc7c4d9e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = CausalMultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward_net = FeedForward(embedding_dim)\n",
    "        self.dynamic_tanh1 = DynamicTanh(embedding_dim)\n",
    "        self.dynamic_tanh2 = DynamicTanh(embedding_dim)\n",
    "        \n",
    "    def forward(self, x, kv_cache = None):\n",
    "        out, kv_cache = self.multi_head_attention(self.dynamic_tanh1(x), kv_cache)\n",
    "        out = x + out\n",
    "        out = out + self.feed_forward_net(self.dynamic_tanh2(out))\n",
    "        \n",
    "        return out, kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b27d7cf-6239-4118-a3cc-e1b0c7af600a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 64, num_heads: int = 8, num_blocks = 8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.dynamic_tanh = DynamicTanh(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, tokens, targets=None, kv_cache = None):\n",
    "\n",
    "        if kv_cache is None:\n",
    "            kv_cache = [None] * len(self.blocks)\n",
    "            out = self.pos_embedding(torch.arange(tokens.shape[-1], device=device)) + self.embedding(tokens)\n",
    "        else:\n",
    "            # trim the kv_cache to keep the context valid\n",
    "            T_past = kv_cache[0][0].shape[2]\n",
    "            if T_past >= context_length:\n",
    "                trim = lambda past_kv : (past_kv[0][:, :, -(context_length-1):, :], past_kv[1][:, :, -(context_length-1):, :])  \n",
    "                kv_cache = [trim(kv_cache[i]) for i in range(len(self.blocks))] \n",
    "            \n",
    "            tokens = tokens[:, [-1]]\n",
    "            out = self.pos_embedding(torch.arange(T_past, T_past+1, device=device)) + self.embedding(tokens)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, updated_block_cache = block(out, kv_cache[i])\n",
    "            kv_cache[i] = updated_block_cache\n",
    "            \n",
    "        out = self.dynamic_tanh(out)\n",
    "        \n",
    "        # If no targets, it is inference and we only care about the last token\n",
    "        if targets is None:    \n",
    "            out = out[:, [-1], :]\n",
    "            \n",
    "        logits = self.lm_head(out)\n",
    "                                 \n",
    "        if targets is None:\n",
    "            return logits, kv_cache\n",
    "            \n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, tokens, temperature = 1, top_k = None, max_new_tokens=100, use_cache = True):\n",
    "        assert temperature>0, \"temperature needs to be positive for comprehensible generations\"\n",
    "        if top_k is not None:\n",
    "            assert top_k>0 and isinstance(top_k, int), \"Non-positive or non-int top_k doesn't make sense!\"\n",
    "        \n",
    "        kv_cache = None\n",
    "        for _ in range(max_new_tokens): \n",
    "            context = tokens[:,-context_length:]\n",
    "            \n",
    "            if use_cache is False:\n",
    "                logits, _ = self(context, None)\n",
    "            else:\n",
    "                logits, kv_cache = self(context, None, kv_cache)\n",
    "            \n",
    "            logits = logits[:,-1,:]/temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                logits = self._get_topk_logits(logits, top_k)\n",
    "                \n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def _get_topk_logits(self, logits, k):\n",
    "        v, _ = torch.topk(logits, k, dim=-1)\n",
    "        min_values = v[:, -1].unsqueeze(-1).expand_as(logits)\n",
    "        \n",
    "        return torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "#     def _get_topk_logits(self, logits, k: int):\n",
    "#         heap = []\n",
    "        \n",
    "#         for logit in logits:\n",
    "#             heappush(heap, logit)\n",
    "#             if len(heap)>k:\n",
    "#                 heappop(heap)\n",
    "           \n",
    "#         logits[logits<heap[0]] = -torch.inf\n",
    "        \n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44802b83-d127-4bd5-8ed3-1a306b4c6f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT(embedding_dim=embedding_dim, num_heads=num_heads, num_blocks=num_blocks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48ed285-a257-4587-b662-602c846548c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(100, 768)\n",
       "  (pos_embedding): Embedding(800, 768)\n",
       "  (blocks): ModuleList(\n",
       "    (0-9): 10 x DecoderBlock(\n",
       "      (multi_head_attention): CausalMultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward_net): FeedForward(\n",
       "        (up_proj): Linear(in_features=768, out_features=4096, bias=False)\n",
       "        (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dynamic_tanh1): DynamicTanh()\n",
       "      (dynamic_tanh2): DynamicTanh()\n",
       "    )\n",
       "  )\n",
       "  (dynamic_tanh): DynamicTanh()\n",
       "  (lm_head): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075f1fc4-abc3-4ce9-b4c0-78b4b7fd0885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 71.61M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e6 :.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4088650d-7c75-42e9-8a18-080cc160f55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df821cb-9e95-4945-901f-750e005ab08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_loss = torch.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14736338-eab4-482b-bae1-52ea52958d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"C:/Users/dhars/Documents/Sagemaker notebooks/GPT1-71M-wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3273058b-6211-4392-96c1-ba725c73be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b887c431-9380-4eca-93df-2726f88e4606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecea0d5-64f8-4468-9b92-40c3d7326c9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for iter_ in tqdm(range(1, max_iters+1), colour='green'):\n",
    "    model.train()\n",
    "    \n",
    "    x,y = get_batches(data=train, batch_size=batch_size, context_length=context_length, device=device)    \n",
    "    \n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(tokens=x,targets=y)\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter_%100==0:\n",
    "        print(f\"[{iter_}/{max_iters}]: Train loss: {loss.mean(): .2f}\")\n",
    "    if iter_%eval_interval==0 or iter_==max_iters:\n",
    "        model.eval()\n",
    "        eval_losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(eval_iters):\n",
    "                x,y = get_batches(test, batch_size, context_length, device)\n",
    "                logits, loss = model(x,y)\n",
    "                eval_losses[i] = loss\n",
    "            eval_loss = eval_losses.mean()\n",
    "            if eval_loss<min_loss:\n",
    "                min_loss = eval_loss\n",
    "                print(f\"Eval loss improved: {eval_loss: .2f}, saving checkpoint\")\n",
    "                torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fe6daf2-e9b7-487d-9d75-dd7b6e8e86af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Eval loss:  1.16\n"
     ]
    }
   ],
   "source": [
    "print(f'Min Eval loss: {min_loss: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e14a486-4b2e-41fc-8430-0086ec061c2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gpt_response(prompt: str, max_new_tokens: int =400, use_cache = True) -> str:\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = torch.tensor(tokenizer.tokenize(prompt), device=device).unsqueeze(0)\n",
    "    out = tokenizer.decode(model.generate(prompt, max_new_tokens=max_new_tokens, use_cache=use_cache)[0].tolist())\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9925bfc-5da4-4f4d-9825-8bdef5b4f9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Turing was born as the 24th president of Sony. He made captains during public mosquititions from a private model during a bridge burning burning in 1854. Its market was high in World War II. It was unnocciated in Srvestle. This was called \"GBD!\". Missing transportation. The 28th Century replaced the Public War. The security is when hands so that the public British was adopted by a variety of books called \"\"The World \"Hell: Editzure Eug\" talking to \"Du Checkpins\"\". Where gy not of the their mechix ovar jert\n"
     ]
    }
   ],
   "source": [
    "print(get_gpt_response(\"Alan Turing was \", max_new_tokens=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba45db3-1e16-4d21-9917-ad30d9813396",
   "metadata": {},
   "source": [
    "### KV cache speed up on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bdae6b79-5e1d-4c74-ae68-cedc7645c247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.247228200081736"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=100), number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fe51c3ed-c724-4aa8-9bb2-4da175b3afbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.07912310003303"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=100, use_cache=False), number=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de4d70-440a-44f8-9d3b-72a2c7d72811",
   "metadata": {},
   "source": [
    "### KV cache speed up on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68fe9b00-9536-4877-96cb-8b659ba71ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.50981409987435"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=750), number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a956c2-69e2-4253-b9e2-d72d187bf689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.67140390002169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit(lambda : get_gpt_response(\"Indian subcontinent\", max_new_tokens=750, use_cache=False), number=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
