Implemented and pre-trained GPT from scratch, trained on some Shakespeare(ref Andrej Karpathy's [tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)). Some minor details might differ.
Trained with DynamicTanh instead of LayerNorm and SwiGLU activation instead of ReLU as well. DyTanh is from a [2025 paper](https://arxiv.org/pdf/2503.10622) co-authored by Yann Lecunn, aims to reduce computational cost for maintained performance.
